{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from w2s.ds_registry import load_and_process_dataset, VALID_DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anli-r2 has < 5000 docs after balancing, using all 668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 35.3k/35.3k [00:00<00:00, 54.6MB/s]\n",
      "Downloading data: 100%|██████████| 251k/251k [00:00<00:00, 2.72MB/s]\n",
      "Downloading data: 100%|██████████| 37.6k/37.6k [00:00<00:00, 849kB/s]\n",
      "Downloading data: 100%|██████████| 37.7k/37.7k [00:00<00:00, 800kB/s]\n",
      "Generating train split: 100%|██████████| 8551/8551 [00:00<00:00, 1323694.17 examples/s]\n",
      "Generating validation split: 100%|██████████| 1043/1043 [00:00<00:00, 738339.08 examples/s]\n",
      "Generating test split: 100%|██████████| 1063/1063 [00:00<00:00, 716002.11 examples/s]\n",
      "Map: 100%|██████████| 8551/8551 [00:00<00:00, 13944.56 examples/s]\n",
      "Filter: 100%|██████████| 8551/8551 [00:00<00:00, 289546.08 examples/s]\n",
      "Filter: 100%|██████████| 8551/8551 [00:00<00:00, 105277.74 examples/s]\n",
      "Filter: 100%|██████████| 8551/8551 [00:00<00:00, 103373.67 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola has < 11000 docs after balancing, using all 5056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5056/5056 [00:00<00:00, 12693.67 examples/s]\n",
      "Map: 100%|██████████| 1043/1043 [00:00<00:00, 16342.50 examples/s]\n",
      "Filter: 100%|██████████| 1043/1043 [00:00<00:00, 191871.01 examples/s]\n",
      "Filter: 100%|██████████| 1043/1043 [00:00<00:00, 89082.41 examples/s]\n",
      "Filter: 100%|██████████| 1043/1043 [00:00<00:00, 90076.58 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cola has < 5000 docs after balancing, using all 644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 644/644 [00:00<00:00, 13248.05 examples/s]\n",
      "/home/adam/.local/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for dream contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/dream\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 4.36k/4.36k [00:00<00:00, 24.0MB/s]\n",
      "Downloading readme: 100%|██████████| 3.79k/3.79k [00:00<00:00, 33.5MB/s]\n",
      "Downloading data: 3.36MB [00:00, 104MB/s]                   \n",
      "Downloading data: 1.10MB [00:00, 107MB/s]                   \n",
      "Downloading data: 1.10MB [00:00, 109MB/s]                   \n",
      "Generating train split: 100%|██████████| 6116/6116 [00:00<00:00, 17972.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 2040/2040 [00:00<00:00, 22879.92 examples/s]\n",
      "Generating test split: 100%|██████████| 2041/2041 [00:00<00:00, 22792.64 examples/s]\n",
      "Map: 100%|██████████| 6116/6116 [00:00<00:00, 10692.98 examples/s]\n",
      "Filter: 100%|██████████| 6116/6116 [00:00<00:00, 75371.65 examples/s]\n",
      "Filter: 100%|██████████| 6116/6116 [00:00<00:00, 44336.15 examples/s]\n",
      "Filter: 100%|██████████| 6116/6116 [00:00<00:00, 42744.25 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dream has < 11000 docs after balancing, using all 6082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 6082/6082 [00:00<00:00, 9950.50 examples/s] \n",
      "Map: 100%|██████████| 2041/2041 [00:00<00:00, 10576.34 examples/s]\n",
      "Filter: 100%|██████████| 2041/2041 [00:00<00:00, 71345.85 examples/s]\n",
      "Filter: 100%|██████████| 2041/2041 [00:00<00:00, 43230.20 examples/s]\n",
      "Filter: 100%|██████████| 2041/2041 [00:00<00:00, 43557.06 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dream has < 5000 docs after balancing, using all 1984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1984/1984 [00:00<00:00, 10891.63 examples/s]\n",
      "Downloading data: 100%|██████████| 716k/716k [00:00<00:00, 2.29MB/s]\n",
      "Downloading data: 100%|██████████| 137k/137k [00:00<00:00, 418kB/s]\n",
      "Downloading data: 100%|██████████| 132k/132k [00:00<00:00, 560kB/s]\n",
      "Generating train split: 100%|██████████| 18164/18164 [00:00<00:00, 1618622.80 examples/s]\n",
      "Generating validation split: 100%|██████████| 3596/3596 [00:00<00:00, 1281672.09 examples/s]\n",
      "Generating test split: 100%|██████████| 3536/3536 [00:00<00:00, 1122544.58 examples/s]\n",
      "Map: 100%|██████████| 18164/18164 [00:01<00:00, 16531.22 examples/s]\n",
      "Filter: 100%|██████████| 18164/18164 [00:00<00:00, 271455.48 examples/s]\n",
      "Filter: 100%|██████████| 18164/18164 [00:00<00:00, 100428.87 examples/s]\n",
      "Filter: 100%|██████████| 18164/18164 [00:00<00:00, 100894.37 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 13542.26 examples/s]\n",
      "Map: 100%|██████████| 3536/3536 [00:00<00:00, 16595.75 examples/s]\n",
      "Filter: 100%|██████████| 3536/3536 [00:00<00:00, 240455.57 examples/s]\n",
      "Filter: 100%|██████████| 3536/3536 [00:00<00:00, 96771.84 examples/s]\n",
      "Filter: 100%|██████████| 3536/3536 [00:00<00:00, 97477.20 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics-deontology has < 5000 docs after balancing, using all 3526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3526/3526 [00:00<00:00, 11107.02 examples/s]\n",
      "Downloading data: 100%|██████████| 894k/894k [00:00<00:00, 2.87MB/s]\n",
      "Downloading data: 100%|██████████| 105k/105k [00:00<00:00, 392kB/s]\n",
      "Downloading data: 100%|██████████| 81.3k/81.3k [00:00<00:00, 327kB/s]\n",
      "Generating train split: 100%|██████████| 21791/21791 [00:00<00:00, 1823948.88 examples/s]\n",
      "Generating validation split: 100%|██████████| 2704/2704 [00:00<00:00, 1037924.23 examples/s]\n",
      "Generating test split: 100%|██████████| 2052/2052 [00:00<00:00, 1111691.01 examples/s]\n",
      "Map: 100%|██████████| 21791/21791 [00:01<00:00, 19746.42 examples/s]\n",
      "Filter: 100%|██████████| 21791/21791 [00:00<00:00, 330722.28 examples/s]\n",
      "Filter: 100%|██████████| 21791/21791 [00:00<00:00, 119538.99 examples/s]\n",
      "Filter: 100%|██████████| 21791/21791 [00:00<00:00, 118637.17 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 14075.33 examples/s]\n",
      "Map: 100%|██████████| 2052/2052 [00:00<00:00, 19028.47 examples/s]\n",
      "Filter: 100%|██████████| 2052/2052 [00:00<00:00, 254005.19 examples/s]\n",
      "Filter: 100%|██████████| 2052/2052 [00:00<00:00, 110478.43 examples/s]\n",
      "Filter: 100%|██████████| 2052/2052 [00:00<00:00, 111180.59 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics-justice has < 5000 docs after balancing, using all 2004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2004/2004 [00:00<00:00, 14348.14 examples/s]\n",
      "Downloading data: 100%|██████████| 762k/762k [00:00<00:00, 2.18MB/s]\n",
      "Downloading data: 100%|██████████| 127k/127k [00:00<00:00, 397kB/s]\n",
      "Downloading data: 100%|██████████| 123k/123k [00:00<00:00, 432kB/s]\n",
      "Generating train split: 100%|██████████| 28245/28245 [00:00<00:00, 2216345.81 examples/s]\n",
      "Generating validation split: 100%|██████████| 4975/4975 [00:00<00:00, 1971901.57 examples/s]\n",
      "Generating test split: 100%|██████████| 4780/4780 [00:00<00:00, 1959801.87 examples/s]\n",
      "Map: 100%|██████████| 28245/28245 [00:01<00:00, 18514.70 examples/s]\n",
      "Filter: 100%|██████████| 28245/28245 [00:00<00:00, 332775.61 examples/s]\n",
      "Filter: 100%|██████████| 28245/28245 [00:00<00:00, 122268.34 examples/s]\n",
      "Filter: 100%|██████████| 28245/28245 [00:00<00:00, 117947.97 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics-virtue has < 11000 docs after balancing, using all 4830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4830/4830 [00:00<00:00, 15163.28 examples/s]\n",
      "Map: 100%|██████████| 4780/4780 [00:00<00:00, 18888.95 examples/s]\n",
      "Filter: 100%|██████████| 4780/4780 [00:00<00:00, 301295.02 examples/s]\n",
      "Filter: 100%|██████████| 4780/4780 [00:00<00:00, 119682.02 examples/s]\n",
      "Filter: 100%|██████████| 4780/4780 [00:00<00:00, 115975.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics-virtue has < 5000 docs after balancing, using all 1912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1912/1912 [00:00<00:00, 14491.08 examples/s]\n",
      "Downloading data: 100%|██████████| 1.40M/1.40M [00:00<00:00, 4.68MB/s]\n",
      "Downloading data: 100%|██████████| 438k/438k [00:00<00:00, 1.64MB/s]\n",
      "Downloading data: 100%|██████████| 413k/413k [00:00<00:00, 1.59MB/s]\n",
      "Generating train split: 100%|██████████| 13737/13737 [00:00<00:00, 1483067.03 examples/s]\n",
      "Generating validation split: 100%|██████████| 4807/4807 [00:00<00:00, 1385134.61 examples/s]\n",
      "Generating test split: 100%|██████████| 4271/4271 [00:00<00:00, 1275735.11 examples/s]\n",
      "Map: 100%|██████████| 13737/13737 [00:00<00:00, 18074.39 examples/s]\n",
      "Filter: 100%|██████████| 13737/13737 [00:00<00:00, 276801.94 examples/s]\n",
      "Filter: 100%|██████████| 13737/13737 [00:00<00:00, 113075.92 examples/s]\n",
      "Filter: 100%|██████████| 13737/13737 [00:00<00:00, 115196.52 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 14980.62 examples/s]\n",
      "Map: 100%|██████████| 4271/4271 [00:00<00:00, 17934.98 examples/s]\n",
      "Filter: 100%|██████████| 4271/4271 [00:00<00:00, 262628.24 examples/s]\n",
      "Filter: 100%|██████████| 4271/4271 [00:00<00:00, 110935.55 examples/s]\n",
      "Filter: 100%|██████████| 4271/4271 [00:00<00:00, 111760.53 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ethics-utilitarianism has < 5000 docs after balancing, using all 4166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4166/4166 [00:00<00:00, 14597.59 examples/s]\n",
      "/home/adam/.local/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for mc_taco contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/mc_taco\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mc_taco has < 11000 docs after balancing, using all 6396\n",
      "mc_taco has < 5000 docs after balancing, using all 2458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 24.4M/24.4M [00:00<00:00, 42.0MB/s]\n",
      "Downloading data: 100%|██████████| 6.11M/6.11M [00:00<00:00, 16.5MB/s]\n",
      "Downloading data: 100%|██████████| 6.32M/6.32M [00:00<00:00, 15.5MB/s]\n",
      "Generating train split: 100%|██████████| 39905/39905 [00:00<00:00, 346285.64 examples/s]\n",
      "Generating test split: 100%|██████████| 10003/10003 [00:00<00:00, 378705.29 examples/s]\n",
      "Generating validation split: 100%|██████████| 10042/10042 [00:00<00:00, 357505.91 examples/s]\n",
      "Map: 100%|██████████| 39905/39905 [00:04<00:00, 8681.59 examples/s]\n",
      "Filter: 100%|██████████| 39905/39905 [00:00<00:00, 83930.25 examples/s]\n",
      "Filter: 100%|██████████| 39905/39905 [00:01<00:00, 38564.48 examples/s]\n",
      "Filter: 100%|██████████| 39905/39905 [00:01<00:00, 38432.84 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:01<00:00, 8441.44 examples/s]\n",
      "Map: 100%|██████████| 10042/10042 [00:01<00:00, 8605.91 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 81640.76 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 37624.00 examples/s]\n",
      "Filter: 100%|██████████| 10042/10042 [00:00<00:00, 37588.91 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 8844.34 examples/s]\n",
      "/home/adam/.local/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for super_glue contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/super_glue\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|██████████| 1.12M/1.12M [00:00<00:00, 4.59MB/s]\n",
      "Generating train split: 100%|██████████| 27243/27243 [00:00<00:00, 29227.86 examples/s]\n",
      "Generating validation split: 100%|██████████| 4848/4848 [00:00<00:00, 32460.21 examples/s]\n",
      "Generating test split: 100%|██████████| 9693/9693 [00:00<00:00, 34204.75 examples/s]\n",
      "Map: 100%|██████████| 27243/27243 [00:02<00:00, 9535.30 examples/s] \n",
      "Filter: 100%|██████████| 27243/27243 [00:00<00:00, 107653.76 examples/s]\n",
      "Filter: 100%|██████████| 27243/27243 [00:00<00:00, 52430.63 examples/s]\n",
      "Filter: 100%|██████████| 27243/27243 [00:00<00:00, 54217.77 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:01<00:00, 10882.55 examples/s]\n",
      "Map: 100%|██████████| 4848/4848 [00:00<00:00, 10142.93 examples/s]\n",
      "Filter: 100%|██████████| 4848/4848 [00:00<00:00, 106746.25 examples/s]\n",
      "Filter: 100%|██████████| 4848/4848 [00:00<00:00, 54901.82 examples/s]\n",
      "Filter: 100%|██████████| 4848/4848 [00:00<00:00, 55196.90 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multirc has < 5000 docs after balancing, using all 4150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4150/4150 [00:00<00:00, 11796.14 examples/s]\n",
      "Downloading readme: 100%|██████████| 9.06k/9.06k [00:00<00:00, 27.6MB/s]\n",
      "Downloading data: 100%|██████████| 496k/496k [00:00<00:00, 8.22MB/s]\n",
      "Downloading data: 100%|██████████| 58.2k/58.2k [00:00<00:00, 1.20MB/s]\n",
      "Downloading data: 100%|██████████| 55.5k/55.5k [00:00<00:00, 1.12MB/s]\n",
      "Generating train split: 100%|██████████| 4957/4957 [00:00<00:00, 740267.92 examples/s]\n",
      "Generating validation split: 100%|██████████| 500/500 [00:00<00:00, 300451.58 examples/s]\n",
      "Generating test split: 100%|██████████| 500/500 [00:00<00:00, 308404.71 examples/s]\n",
      "Map: 100%|██████████| 4957/4957 [00:00<00:00, 10649.99 examples/s]\n",
      "Filter: 100%|██████████| 4957/4957 [00:00<00:00, 76035.29 examples/s]\n",
      "Filter: 100%|██████████| 4957/4957 [00:00<00:00, 47463.54 examples/s]\n",
      "Filter: 100%|██████████| 4957/4957 [00:00<00:00, 28432.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa has < 11000 docs after balancing, using all 4912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4912/4912 [00:00<00:00, 13315.47 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:00<00:00, 9468.38 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 58073.55 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 40303.49 examples/s]\n",
      "Filter: 100%|██████████| 500/500 [00:00<00:00, 40988.82 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openbookqa has < 5000 docs after balancing, using all 472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 472/472 [00:00<00:00, 12737.57 examples/s]\n",
      "/home/adam/.local/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for piqa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/piqa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 5.36k/5.36k [00:00<00:00, 19.0MB/s]\n",
      "Downloading readme: 100%|██████████| 8.41k/8.41k [00:00<00:00, 27.4MB/s]\n",
      "Downloading data: 100%|██████████| 1.82M/1.82M [00:00<00:00, 24.5MB/s]\n",
      "Downloading data: 100%|██████████| 815k/815k [00:00<00:00, 83.7MB/s]\n",
      "Generating train split: 100%|██████████| 16113/16113 [00:00<00:00, 46475.47 examples/s]\n",
      "Generating test split: 100%|██████████| 3084/3084 [00:00<00:00, 50268.67 examples/s]\n",
      "Generating validation split: 100%|██████████| 1838/1838 [00:00<00:00, 46729.08 examples/s]\n",
      "Map: 100%|██████████| 16113/16113 [00:01<00:00, 14424.33 examples/s]\n",
      "Filter: 100%|██████████| 16113/16113 [00:00<00:00, 213740.49 examples/s]\n",
      "Filter: 100%|██████████| 16113/16113 [00:00<00:00, 83548.11 examples/s]\n",
      "Filter: 100%|██████████| 16113/16113 [00:00<00:00, 83735.69 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 12199.44 examples/s]\n",
      "Map: 100%|██████████| 1838/1838 [00:00<00:00, 15418.66 examples/s]\n",
      "Filter: 100%|██████████| 1838/1838 [00:00<00:00, 181557.92 examples/s]\n",
      "Filter: 100%|██████████| 1838/1838 [00:00<00:00, 79715.54 examples/s]\n",
      "Filter: 100%|██████████| 1838/1838 [00:00<00:00, 80412.34 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piqa has < 5000 docs after balancing, using all 1832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1832/1832 [00:00<00:00, 12882.66 examples/s]\n",
      "Downloading readme: 100%|██████████| 8.40k/8.40k [00:00<00:00, 25.7MB/s]\n",
      "Downloading data: 100%|██████████| 1.79M/1.79M [00:00<00:00, 3.71MB/s]\n",
      "Downloading data: 100%|██████████| 398k/398k [00:00<00:00, 1.31MB/s]\n",
      "Downloading data: 100%|██████████| 97.0k/97.0k [00:00<00:00, 368kB/s]\n",
      "Generating train split: 100%|██████████| 10246/10246 [00:00<00:00, 268999.60 examples/s]\n",
      "Generating validation split: 100%|██████████| 2164/2164 [00:00<00:00, 219673.60 examples/s]\n",
      "Generating challenge split: 100%|██████████| 556/556 [00:00<00:00, 206265.08 examples/s]\n",
      "Map: 100%|██████████| 10246/10246 [00:01<00:00, 6394.19 examples/s]\n",
      "Filter: 100%|██████████| 10246/10246 [00:00<00:00, 55458.70 examples/s]\n",
      "Filter: 100%|██████████| 10246/10246 [00:00<00:00, 30626.70 examples/s]\n",
      "Filter: 100%|██████████| 10246/10246 [00:00<00:00, 30830.69 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quail has < 11000 docs after balancing, using all 10226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10226/10226 [00:01<00:00, 7991.48 examples/s]\n",
      "Map: 100%|██████████| 2164/2164 [00:00<00:00, 6260.54 examples/s]\n",
      "Filter: 100%|██████████| 2164/2164 [00:00<00:00, 53145.30 examples/s]\n",
      "Filter: 100%|██████████| 2164/2164 [00:00<00:00, 30369.00 examples/s]\n",
      "Filter: 100%|██████████| 2164/2164 [00:00<00:00, 30459.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quail has < 5000 docs after balancing, using all 2148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2148/2148 [00:00<00:00, 8554.34 examples/s]\n",
      "Downloading readme: 100%|██████████| 8.83k/8.83k [00:00<00:00, 28.6MB/s]\n",
      "Downloading data: 100%|██████████| 415k/415k [00:00<00:00, 1.31MB/s]\n",
      "Downloading data: 100%|██████████| 99.8k/99.8k [00:00<00:00, 409kB/s]\n",
      "Downloading data: 100%|██████████| 54.3k/54.3k [00:00<00:00, 318kB/s]\n",
      "Generating train split: 100%|██████████| 2696/2696 [00:00<00:00, 371394.34 examples/s]\n",
      "Generating test split: 100%|██████████| 784/784 [00:00<00:00, 278577.97 examples/s]\n",
      "Generating validation split: 100%|██████████| 384/384 [00:00<00:00, 145270.38 examples/s]\n",
      "Map: 100%|██████████| 2696/2696 [00:00<00:00, 6378.81 examples/s]\n",
      "Filter: 100%|██████████| 2696/2696 [00:00<00:00, 34628.00 examples/s]\n",
      "Filter: 100%|██████████| 2696/2696 [00:00<00:00, 22529.89 examples/s]\n",
      "Filter: 100%|██████████| 2696/2696 [00:00<00:00, 19601.45 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quartz has < 11000 docs after balancing, using all 2666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2666/2666 [00:00<00:00, 7390.87 examples/s]\n",
      "Map: 100%|██████████| 784/784 [00:00<00:00, 5817.73 examples/s]\n",
      "Filter: 100%|██████████| 784/784 [00:00<00:00, 32943.30 examples/s]\n",
      "Filter: 100%|██████████| 784/784 [00:00<00:00, 17939.83 examples/s]\n",
      "Filter: 100%|██████████| 784/784 [00:00<00:00, 22344.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quartz has < 5000 docs after balancing, using all 764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 764/764 [00:00<00:00, 9222.92 examples/s]\n",
      "/home/adam/.local/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for social_i_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/social_i_qa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|██████████| 4.67k/4.67k [00:00<00:00, 15.3MB/s]\n",
      "Downloading readme: 100%|██████████| 6.81k/6.81k [00:00<00:00, 23.3MB/s]\n",
      "Downloading data: 100%|██████████| 2.20M/2.20M [00:00<00:00, 24.5MB/s]\n",
      "Generating train split: 100%|██████████| 33410/33410 [00:00<00:00, 37928.09 examples/s]\n",
      "Generating validation split: 100%|██████████| 1954/1954 [00:00<00:00, 37509.13 examples/s]\n",
      "Map: 100%|██████████| 33410/33410 [00:03<00:00, 10832.09 examples/s]\n",
      "Filter: 100%|██████████| 33410/33410 [00:00<00:00, 157456.12 examples/s]\n",
      "Filter: 100%|██████████| 33410/33410 [00:00<00:00, 64401.04 examples/s]\n",
      "Filter: 100%|██████████| 33410/33410 [00:00<00:00, 63574.54 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:01<00:00, 10957.05 examples/s]\n",
      "Map: 100%|██████████| 1954/1954 [00:00<00:00, 10581.39 examples/s]\n",
      "Filter: 100%|██████████| 1954/1954 [00:00<00:00, 133769.73 examples/s]\n",
      "Filter: 100%|██████████| 1954/1954 [00:00<00:00, 58794.15 examples/s]\n",
      "Filter: 100%|██████████| 1954/1954 [00:00<00:00, 55367.54 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "social_i_qa has < 5000 docs after balancing, using all 1888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1888/1888 [00:00<00:00, 8152.32 examples/s] \n",
      "Downloading readme: 100%|██████████| 5.27k/5.27k [00:00<00:00, 14.7MB/s]\n",
      "Downloading data: 100%|██████████| 3.11M/3.11M [00:00<00:00, 12.0MB/s]\n",
      "Downloading data: 100%|██████████| 72.8k/72.8k [00:00<00:00, 1.49MB/s]\n",
      "Downloading data: 100%|██████████| 148k/148k [00:00<00:00, 644kB/s]\n",
      "Generating train split: 100%|██████████| 67349/67349 [00:00<00:00, 1673363.58 examples/s]\n",
      "Generating validation split: 100%|██████████| 872/872 [00:00<00:00, 283302.33 examples/s]\n",
      "Generating test split: 100%|██████████| 1821/1821 [00:00<00:00, 740027.86 examples/s]\n",
      "Map: 100%|██████████| 67349/67349 [00:03<00:00, 17168.79 examples/s]\n",
      "Filter: 100%|██████████| 67349/67349 [00:00<00:00, 294112.26 examples/s]\n",
      "Filter: 100%|██████████| 67349/67349 [00:00<00:00, 100747.50 examples/s]\n",
      "Filter: 100%|██████████| 67349/67349 [00:00<00:00, 101853.56 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 13134.06 examples/s]\n",
      "Map: 100%|██████████| 872/872 [00:00<00:00, 17397.46 examples/s]\n",
      "Filter: 100%|██████████| 872/872 [00:00<00:00, 183412.72 examples/s]\n",
      "Filter: 100%|██████████| 872/872 [00:00<00:00, 87108.70 examples/s]\n",
      "Filter: 100%|██████████| 872/872 [00:00<00:00, 87569.63 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sst2 has < 5000 docs after balancing, using all 856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 856/856 [00:00<00:00, 14270.93 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wic has < 11000 docs after balancing, using all 5428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5428/5428 [00:00<00:00, 8640.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wic has < 5000 docs after balancing, using all 638\n",
      "sciq has < 11000 docs after balancing, using all 10674\n",
      "sciq has < 5000 docs after balancing, using all 2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10679/10679 [00:00<00:00, 12165.31 examples/s]\n",
      "Filter: 100%|██████████| 10679/10679 [00:00<00:00, 145884.20 examples/s]\n",
      "Filter: 100%|██████████| 10679/10679 [00:00<00:00, 61199.72 examples/s]\n",
      "Filter: 100%|██████████| 10679/10679 [00:00<00:00, 61897.10 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciq_with_support has < 11000 docs after balancing, using all 10674\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10674/10674 [00:00<00:00, 11822.51 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 11769.22 examples/s]\n",
      "Filter: 100%|██████████| 3000/3000 [00:00<00:00, 136788.62 examples/s]\n",
      "Filter: 100%|██████████| 3000/3000 [00:00<00:00, 61392.04 examples/s]\n",
      "Filter: 100%|██████████| 3000/3000 [00:00<00:00, 59433.26 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sciq_with_support has < 5000 docs after balancing, using all 2980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2980/2980 [00:00<00:00, 11824.94 examples/s]\n",
      "Downloading readme: 100%|██████████| 5.77k/5.77k [00:00<00:00, 28.3MB/s]\n",
      "Downloading data: 100%|██████████| 13.2M/13.2M [00:00<00:00, 46.9MB/s]\n",
      "Downloading data: 100%|██████████| 16.2M/16.2M [00:00<00:00, 203MB/s]\n",
      "Downloading data: 100%|██████████| 20.1M/20.1M [00:00<00:00, 215MB/s]\n",
      "Downloading data: 100%|██████████| 25.7M/25.7M [00:00<00:00, 268MB/s]\n",
      "Downloading data: 100%|██████████| 743k/743k [00:00<00:00, 16.6MB/s]\n",
      "Downloading data: 100%|██████████| 875k/875k [00:00<00:00, 19.1MB/s]\n",
      "Downloading data: 100%|██████████| 1.05M/1.05M [00:00<00:00, 21.6MB/s]\n",
      "Downloading data: 100%|██████████| 1.36M/1.36M [00:00<00:00, 25.1MB/s]\n",
      "Generating train split: 100%|██████████| 160800/160800 [00:01<00:00, 118483.85 examples/s]\n",
      "Generating test split: 100%|██████████| 8552/8552 [00:00<00:00, 124983.23 examples/s]\n",
      "Map: 100%|██████████| 160800/160800 [00:09<00:00, 17655.32 examples/s]\n",
      "Filter: 100%|██████████| 160800/160800 [00:00<00:00, 174049.77 examples/s]\n",
      "Filter: 100%|██████████| 160800/160800 [00:01<00:00, 90623.73 examples/s]\n",
      "Filter: 100%|██████████| 160800/160800 [00:01<00:00, 91213.11 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 14564.42 examples/s]\n",
      "Map: 100%|██████████| 8552/8552 [00:00<00:00, 17799.64 examples/s]\n",
      "Filter: 100%|██████████| 8552/8552 [00:00<00:00, 163995.21 examples/s]\n",
      "Filter: 100%|██████████| 8552/8552 [00:00<00:00, 90177.64 examples/s]\n",
      "Filter: 100%|██████████| 8552/8552 [00:00<00:00, 90924.66 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 13850.44 examples/s]\n",
      "/home/adam/.local/lib/python3.10/site-packages/datasets/load.py:1491: FutureWarning: The repository for cosmos_qa contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/cosmos_qa\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosmos_qa has < 5000 docs after balancing, using all 2646\n",
      "boolq has < 11000 docs after balancing, using all 7106\n",
      "boolq has < 5000 docs after balancing, using all 2474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 6.81k/6.81k [00:00<00:00, 33.5MB/s]\n",
      "Downloading data: 100%|██████████| 260M/260M [00:00<00:00, 300MB/s]  \n",
      "Downloading data: 100%|██████████| 258M/258M [00:00<00:00, 299MB/s]  \n",
      "Downloading data: 100%|██████████| 255M/255M [00:01<00:00, 168MB/s]  \n",
      "Downloading data: 100%|██████████| 254M/254M [00:00<00:00, 334MB/s]  \n",
      "Downloading data: 100%|██████████| 117M/117M [00:00<00:00, 312MB/s]  \n",
      "Generating train split: 100%|██████████| 3600000/3600000 [00:05<00:00, 678121.79 examples/s]\n",
      "Generating test split: 100%|██████████| 400000/400000 [00:00<00:00, 682037.52 examples/s]\n",
      "Map: 100%|██████████| 3600000/3600000 [03:41<00:00, 16225.15 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [00:14<00:00, 251267.07 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [01:41<00:00, 35403.32 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [01:43<00:00, 34858.13 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 12214.73 examples/s]\n",
      "Map: 100%|██████████| 400000/400000 [00:24<00:00, 16094.50 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:01<00:00, 249975.32 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:04<00:00, 93174.35 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:04<00:00, 94241.15 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 14318.76 examples/s]\n",
      "Map: 100%|██████████| 3600000/3600000 [03:39<00:00, 16420.16 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [00:14<00:00, 247558.43 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [01:42<00:00, 35085.15 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [01:43<00:00, 34769.03 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 11526.88 examples/s]\n",
      "Map: 100%|██████████| 400000/400000 [00:24<00:00, 16224.41 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:01<00:00, 247800.49 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:04<00:00, 93971.84 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:04<00:00, 92246.16 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 14188.26 examples/s]\n",
      "Map: 100%|██████████| 3600000/3600000 [03:44<00:00, 16011.57 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [00:14<00:00, 249687.36 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [01:42<00:00, 35176.02 examples/s]\n",
      "Filter: 100%|██████████| 3600000/3600000 [01:43<00:00, 34914.62 examples/s]\n",
      "Map: 100%|██████████| 11000/11000 [00:00<00:00, 12920.24 examples/s]\n",
      "Map: 100%|██████████| 400000/400000 [00:25<00:00, 15890.82 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:01<00:00, 246277.82 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:04<00:00, 93921.95 examples/s]\n",
      "Filter: 100%|██████████| 400000/400000 [00:04<00:00, 93512.92 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 11470.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "loaded_datasets = {\n",
    "    ds: load_and_process_dataset(\n",
    "        ds, 10_000, 1_000, 5_000, 0\n",
    "    )\n",
    "    for ds in VALID_DATASETS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('loaded_datasets.pkl', 'wb') as f:\n",
    "    pickle.dump(loaded_datasets, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ok.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set HF_TOKEN env var to use the HF tokenizers\n",
    "import os\n",
    "os.environ['HF_TOKEN'] = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adam/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-8B is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1722\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1721\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1722\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1723\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1645\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1644\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1645\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1654\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:372\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 372\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:396\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    395\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:367\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    361\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    362\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Forbidden: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_message\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    363\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCannot access content at: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    364\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf you are trying to create or update content,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    365\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmake sure you have a token with the `write` role.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    366\u001b[0m     )\n\u001b[0;32m--> 367\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HfHubHTTPError(message, response\u001b[38;5;241m=\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n",
      "\u001b[0;31mHfHubHTTPError\u001b[0m:  (Request ID: Root=1-6664226a-5f1897073b20269c27352bc5;a277ba95-6db7-46b4-996e-8d6028a5cd03)\n\n403 Forbidden: Authorization error..\nCannot access content at: https://huggingface.co/meta-llama/Meta-Llama-3-8B/resolve/main/config.json.\nIf you are trying to create or update content,make sure you have a token with the `write` role.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:399\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 399\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    400\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    401\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    402\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    403\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    411\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    412\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1221\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1220\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[1;32m   1223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[1;32m   1225\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[1;32m   1230\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1231\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1232\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1234\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[1;32m   1235\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1237\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1325\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, headers, proxies, etag_timeout, endpoint, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1324\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1826\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1824\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1825\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n\u001b[0;32m-> 1826\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LocalEntryNotFoundError(\n\u001b[1;32m   1827\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error happened while trying to locate the file on the Hub and we cannot find the requested files\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1828\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the local cache. Please check your connection and try again or make sure your Internet connection\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1829\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is on.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1830\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhead_call_error\u001b[39;00m\n",
      "\u001b[0;31mLocalEntryNotFoundError\u001b[0m: An error happened while trying to locate the file on the Hub and we cannot find the requested files in the local cache. Please check your connection and try again or make sure your Internet connection is on.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta-llama/Meta-Llama-3-8B\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:837\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    835\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    836\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 837\u001b[0m         config \u001b[38;5;241m=\u001b[39m \u001b[43mAutoConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    840\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:934\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    931\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    932\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 934\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m \u001b[43mPretrainedConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    935\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    936\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:632\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    630\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 632\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_config_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict:\n\u001b[1;32m    634\u001b[0m     original_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/configuration_utils.py:689\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    685\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    688\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 689\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    690\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    691\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfiguration_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    692\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    693\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m     commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m:\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;66;03m# Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# the original exception.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/transformers/utils/hub.py:442\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    437\u001b[0m         resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries\n\u001b[1;32m    439\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_connection_errors\n\u001b[1;32m    440\u001b[0m     ):\n\u001b[1;32m    441\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 442\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWe couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt connect to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to load this file, couldn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt find it in the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    444\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m cached files and it looks like \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not the path to a directory containing a file named\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    445\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_filename\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mCheckout your internet connection or see how to run the library in offline mode at\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/docs/transformers/installation#offline-mode\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    449\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_missing_entries:\n",
      "\u001b[0;31mOSError\u001b[0m: We couldn't connect to 'https://huggingface.co' to load this file, couldn't find it in the cached files and it looks like meta-llama/Meta-Llama-3-8B is not the path to a directory containing a file named config.json.\nCheckout your internet connection or see how to run the library in offline mode at 'https://huggingface.co/docs/transformers/installation#offline-mode'."
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def process(examples):\n",
    "    out = tokenizer(examples[\"txt\"], truncation=True)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = [\n",
    "    (\"boolq\", 2),\n",
    "    (\"anli-r2\", 8),\n",
    "    (\"cosmos_qa\", 4),\n",
    "    (\"mc_taco\", 4),\n",
    "    (\"sciq\", 4),\n",
    "    (\"paws\", 16),\n",
    "    (\"twitter-sentiment\", 8),\n",
    "    (\"wic\", 8),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_size = 0\n",
    "best_ds = None\n",
    "for ds, minibatch in configs:\n",
    "    ds_dict = loaded_datasets[ds]\n",
    "    ds_dict = ds_dict.map(process, batched=True)\n",
    "    for split in ds_dict:\n",
    "        max_len = max(ds_dict[split][\"input_ids\"].apply(lambda x: len(x)))\n",
    "        size = max_len * minibatch\n",
    "        print(ds, split, size)\n",
    "        if size > max_size:\n",
    "            max_size = size\n",
    "            best_ds = ds\n",
    "\n",
    "print(best_ds, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "safe_configs = []\n",
    "\n",
    "for ds in VALID_DATASETS:\n",
    "    ds_dict = loaded_datasets[ds]\n",
    "    ds_dict = ds_dict.map(process, batched=True)\n",
    "    max_len = 0\n",
    "    for split in ds_dict:\n",
    "        max_len = max(max_len, max(ds_dict[split][\"input_ids\"].apply(lambda x: len(x))))\n",
    "    safe_configs.append((ds, max_size // max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(safe_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_dict = loaded_datasets[\"boolq\"]\n",
    "ds_dict = ds_dict.map(process, batched=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ipy310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
